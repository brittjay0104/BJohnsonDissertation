\chapter{Static Analysis Tools Use}
\label{chap:why}

% TODO static analysis --> program analysis ??
One common type of program analysis tool that developers use when writing code is known as a static analysis tool. 
As described in the previous chapter, static analysis tools automate software development tasks to reduce developer effort, especially when completing tasks such as finding and resolving defects.
However, despite the benefits of using static analysis tools to find bugs, consistent
usage of these tools is not very frequent~\cite{Ayewah:2008:FindBugs, ge2012reconciling}.
 
There have been studies to investigate ways of improving static analysis tools~\cite{Bessey:2010:Coverity, Khoo:2008:PathProjection}.
However, none look at what the tools do for a developer, what could be improved \emph{and}
why these improvements might help. The study outlined in this chapter provided an improved understanding 
of why software developers are not using static analysis tools and how
current tools could be improved to increase usage based on developer feedback.

To answer the question \emph{why do developers not use program analysis tools}, I conducted interactive interviews with 20 professional developers to better understand why they do not use static analysis tools to find bugs when writing code~\cite{johnson2013don}.
For this study, I focused on static analysis tools used to finds bugs.
This includes tools like FindBugs, Lint~\cite{Johnson:1978:Lint} ,
IntelliJ~\cite{IntelliJIDEA} (which includes built-in static analyzers), and
PMD~\cite{PMD}. FindBugs will be referenced the most as it is the tool I chose
to use during the interviews.

\section{Exploring Developer Tool Use}

\begin{table}[]
\centering
\caption{Descriptive statistics reported by participants.}
\label{table:demographics}
\rowcolors{2}{gray!25}{white}
\begin{tabular}{llll}
\toprule
\rowcolor{gray!50}
\textbf{Participant} & \textbf{Open Source Tools}                                                                                      & \textbf{Closed Source Tools}              & \textbf{Local} \\
\midrule
Abby                 & FindBugs                                                                                                        & IntelliJ                                  & Yes            \\
Adam                 & CheckStyle, FindBugs, PMD                                                                                       & IntelliJ                                  & Yes            \\
Andy                 & FindBugs, Lint                                                                                                  & Jtest                                     & Yes            \\
Chris                & CheckStyle, FindBugs, Lint                                                                                      & Coverity                                  & Yes            \\
Cody                 & Dehydra                                                                                                         & -                                         & Yes            \\
Frank                & -                                                                                                               & -                                         & Yes            \\
Gordon               & Lint, CheckStyle, FindBugs                                                                                      & -                                         & Yes            \\
Jake                 & FindBugs, Lint                                                                                                  & FlexLint, Klocwork Insight, Visual Studio & Yes            \\
James                & CheckStyle, FindBugs, Lint                                                                                      & Visual Studio                             & Yes            \\
Jason                & FindBugs, Lint                                                                                                  & -                                         & Yes            \\
John                 & \begin{tabular}[c]{@{}l@{}}CheckStyle, Copy/Paste Detector(CPD), \\ FindBugs, Lint, PMD \& CodePro\end{tabular} & -                                         & Yes            \\
Jordan               & CheckStyle, FindBugs, PMD                                                                                       & JTest                                     & Yes            \\
Josh                 & FindBugs, Lint                                                                                                  & Coverity                                  & No             \\
Lee                  & CheckStyle, FindBugs, Lint                                                                                      & Visual Studio                             & Yes            \\
Matt                 & Lint                                                                                                            & FlexLint, PyCharm                         & Yes            \\
Phil                 & -                                                                                                               & -                                         & Yes            \\
Ray                  & CheckStyle, FindBugs                                                                                            & -                                         & Yes            \\
Ryan                 & FindBugs, Ling                                                                                                  & Coverity                                  & Yes            \\
Steve                & CheckStyle, FindBugs, Lint, CPD                                                                                 & IntelliJ                                  & Yes            \\
Tony                 & \begin{tabular}[c]{@{}l@{}}CheckStyle,  FindBugs, Lint, PMD,\\ CPD, cpplint, Splint\end{tabular}                & Coverity                                  &               Yes \\
\bottomrule
\end{tabular}
\end{table}

For this study, I conducted interviews with software developers. Each
semi-structured interview lasted approximately 40-60 minutes and, with the
participant's consent, was recorded. By conducting ``semi-structured''
interviews, I aimed to achieve the flexibility needed to get as much detailed
information as possible~\cite{Hove:2005:Interview}. I prepared a script of
questions for the interview, but would add or omit questions on the fly
depending on how detailed a participant was in their responses.
We created and modified the script as we conducted trial interviews; any changes
made to the script was based on the responses we got from our 4 trial
participants~\cite{Johnson:2012:PreFFSAT}.


Upon completion, I manually transcribed each session. I
performed qualitative analysis \footnote{All study materials including interview
scripts and coding categories are available at
\url{http://www4.ncsu.edu/~bijohnso/ffsat.html}} on the transcripts by
``coding'' the transcriptions. This process is discussed in detail in
Section~\ref{subsec:coding}.

\subsection{Participants}
\label{subsec:participants}

I conducted this study with a group of 20 participants. Although this seems
like a small sample, I followed a similar methodology to that of Layman et.
al.'s study that only had 18 participants~\cite{Layman:2007:FaultFix}.
I recruited participants using an electronic recruitment flyer that was sent
out to industry contacts to then be sent to developers within their company.
Sixteen participants were, at the time, professional developers at a large company and 4
were graduate students at North Carolina State University with previous industry
experience. Participants' years of development experience ranged from 3 to 25
years. We did not explicitly ask participants about their experience building
static analysis tools, however, based on conversations approximately 2
participants had tool building experience. We interviewed two participants
remotely, one by phone and one by video chat, due to location differences. Each
participant filled out a short questionnaire used to collect demographic
information prior to their interview.

Table~\ref{table:demographics} shows the statistics and background information
gathered from the questionnaire and interviews. The first column lists the
participants' pseudonyms, given for confidentiality purposes. The second and
third columns show the open-source tools and closed-source tools that they have
used to find bugs. If a space has a ``-'', it indicates no response from the
participant.  


\subsection{Research Questions}
\label{subsec:rq}
For this study, I answered the following research questions:
\begin{itemize}
\item [RQ\textsubscript{1}]: What reasons do developers have for using
or not using static analysis tools to find bugs?
\item [RQ\textsubscript{2}]: How well do current static analysis tools
fit into the workflows of developers? We define a workflow as the steps a
developer takes when writing, inspecting and modifying their code.
\item [RQ\textsubscript{3}]: What improvements do developers want
to see being made to static analysis tools?
\end{itemize}

I asked these questions because answers to these questions are important to the progression through my dissertation and can give toolsmiths
and researchers areas for future work and improvement in the area of static
analysis tool usability. Research has shown that the way a tool interrupts a developer's
workflow is important therefore I also wanted to specifically investigate this
aspect of tool usage~\cite{Robertson:2004:Interruption, Gluck:2007:Attentional}.
The interviews focused on developers' experiences with finding defects using
static analysis tools. Learning developers' relevant experiences and observing
how they use static analysis tools to find bugs may shed some light on why these
tools are underused. I organized the interviews into into three main
parts: Questions and Short Responses (Section~\ref{subsec:part1}), Interactive
Interview (Section~\ref{subsec:part2}), and Participatory Design
(Section~\ref{subsec:part3}).


\subsection{Part I: Questions and Short Responses}
\label{subsec:part1}
During part 1, Question and Short Response, I asked developers questions
related to their general usage, understanding, and opinion of static analysis
tools in order to answer~\hyperlink{RQ1}{RQ1}.

Some of the questions I asked include:
\begin{itemize}
\item Can you tell me about your first experience with a static analysis tool?
\item Can you remember anything that stood out about this experience as easy or difficult?
\item Have you ever used a static analysis tool in a team setting? Was it beneficial and why?
\item Have you ever consciously avoided using a static analysis tool? Why or why not?
\item What in your opinion are the critical characteristics of a good static analysis tool?
\end{itemize}

\subsection{Part II: Interactive Interview}
\label{subsec:part2}
The second part is what I called the ``interactive interview''. The goal behind the
interactive interview is to be able to observe developers actually using a
static analysis tool. This allowed for the gathering of more detailed information as to how
developers are using their tools. I used the information obtained during
this portion to address~\hyperlink{RQ2}{RQ2}. I asked participants to
explain what they are doing out loud~\cite{Lewis:1982:ThinkAloudProtocol} so I
could get a better understanding of their workflow and thought process. Practice
interviews before this study revealed that using the interactive interview
portion produced more detailed information regarding when and how developers use
their static analysis tools~\cite{Johnson:2012:PreFFSAT}.

Some of the questions asked during this portion include:
\begin{itemize}
\item Now that you have run your tool and gotten your feedback, what is your next move(s)?
\item Do you configure the settings of your tool from default? If so, how?
\item Does this static analysis tool aid in assessing what to do about a warning?
\item Do you feel that ``quick fixes'' or code suggestions would be helpful if
they were available?\footnote{Participants were only asked about quick fixes and
code suggestions being useful when they mentioned, either during the Question
and Answer or Interactive Interview, that they either a) find quick fixes
useful, b) felt that the tool should be more helpful or c) did not understand
how to fix the defect we presented them with. }
\end{itemize}

For confidentiality reasons, not all participants could use their own
workstation for this part of the interview. For those who could not, I provided
6 open source projects in Java, such as log4j~\cite{log4j} and Ant~\cite{ANT},
and asked each participant to run FindBugs on one of them.  I chose FindBugs
because it is one of the most popular and mature open source static analysis tools for
Eclipse. Due to technical difficulties, the remote interviews could not fully experience the ``interactive'' portion. 
Each remote participant was given a scenario of static analysis tool usage and asked to, first, explain their thought process in
walking through that particular scenario. I then asked the same questions as I would have asked if they had been local.


\subsection{Part III: Participatory Design}
\label{subsec:part3}

The last part of the interview allowed participants to make
design suggestions for improving static analysis tools. I utilized a concept
called \textit{participatory design}~\cite{Spinuzzi:2005:Participatory}, which involves
getting stakeholders (in this case, the participants) involved in the design
process by allowing them to show what they want instead of saying it. In order
to promote creativity, each participant was given a blank sheet of paper and
asked to sketch what they wanted their tool to look like and describe how it should
work~\cite{Johnson:2012:PreFFSAT}. I did not require participants to draw
something, but 6 of them did. The rest of the participants gave verbal
descriptions of tool features they desired.


\subsection{Coding Interview Responses}
\label{subsec:coding}
After completing the interviews, I manually transcribed each interview.
Then, I \textit{coded} the transcriptions. Coding
is a process that is meant to make referencing transcriptions quicker and
easier~\cite{Gordon:1998:Coding}. I used Gordon's basic steps to code the 
interview transcripts and used the codes to help organize the Results
(Section~\ref{sec:barriers}). According to Gordon's steps, before coding an interview, ``coding
categories'' need to be defined. These should be general enough for relevant
information to be grouped together but detailed enough that a concrete example
only falls under one category. Because of this, it is possible to have
``emergent'' categories that may need to be defined after reading the
transcriptions. I developed and used the following coding categories:

\begin{itemize}
    \item \emph{Tool Output}: anything related to the output produced by
the tool (for example, false positives).
    \item \emph{Supporting Teamwork}: anything about using static analysis tools in a team or collaborative setting
    \item \emph{User Input and Customizability}: points made about the customizability of the static analysis tools (for example, modifying rule sets)
    \item \emph{Result Understandability}:  anything said about the ability or inability to understand or interpret the results produced by a static analysis tool
    \item \emph{Workflows}: anything related to the steps a developer takes when writing, inspecting and modifying their software (for example, tool integration)
    \item \emph{Tool Design}: the proposed tool design ideas from our participants.
\end{itemize}

\noindent Examples of each of these categories from the transcriptions are as follows:
\begin{description}
\item[Tool Output] \hfill \\
    Jason: \emph{``\ldots like I mentioned with FlexLint it gives you so many warnings and sifting through them is so, arduous that whenever I just
look at it I'm like ehhh forget this.''}

\item[User Input/Customizability] \hfill \\ 
	Andy: \emph{``\ldots it's like is this
list prioritized by you know what's important to me? No. You know? And there
may be a default listing that should be prioritized because like this one's
inefficient.''}
\item[Supporting Teamwork] \hfill \\
	John: \emph{``The only reason I like the batch
results is to communicate, broadcast to the team a sense of progress or lack of
progress.''}
\item[Result Understandability] \hfill \\ 
	Matt: \emph{so now I wanna know why raising a string
exception is bad. Like what should I be doing instead? Since it thinks it's a
problem. And so none of these really help me.}
\item[Workflows] \hfill \\
	Mike: \emph{``Clang is my favorite. It's built into the compiler. You don't
have to invoke anything special.''}
\item[Tool Design] \hfill \\
	Chris: \emph{``I don't mind the idea of the actual source code itself having
some plasticity \ldots let's say the fourth line there was some error
here\ldots having the 5th line drop down and having the content expand with maybe all
sorts of annotations about my code.''}
\end{description}

The next step in Gordon's methodology is to assign ``category symbols'' to each
category for easier indexing and processing of information. Gordon then suggests
finding and classifying the relevant information in the transcriptions using the
category symbols. In my set of codes, each coding category had its own color as a
``symbol''; if a portion of a participant's transcription fell into one of the
categories, the text would be highlighted the same color as its respective
category. A participant's coded interview could contain multiple categories or
even multiple data items for one category. To ensure consistency, only I was
responsible for coming up with the coding categories and ``symbols'' and going
through the transcriptions to apply them. The last step is to check the
reliability of the codes. For this study, once the coding was complete, I passed the documents over to other contributors to look over. 
If there were any discrepancies we discussed and resolved them as a group. This includes items
that could fall into more than one category; in this situation, either a new,
more specific, category or a ``sub-category'' was created for the item. The
purpose of the categories are to organize the data in a relevant and useful
manner; they are not meant to directly correlate with the research questions. 
The next section discusses findings from this study.

\section{Barriers to Tool Use}
\label{sec:barriers}

In this section, I will discuss the results obtained from the interviews. I answered my
research questions by linking the questions to coding categories and interview
parts. My first research question (\hyperlink{RQ1}{RQ1}) was answered by observing the results categorized under ``Tool Output,'' ``Supporting Teamwork,'' ``User Input and
Customizability,'' ``Result Understandability'' and ``Developer Workflows''; the information collected in these categories could be reasons why developers are or
are not using static analysis tools. My second research question (\hyperlink{RQ2}{RQ2}) was answered by observing the results categorized under ``Developer Workflows.'' 
My third research question (\hyperlink{RQ3}{RQ3}) was answered by observing the results categorized under ``Tool Design''; most of these results are from the Participatory Design portion.

In each category, I expected there to be negative and positive remarks about
current tools, both of which are equally important in answering my research
questions; anything positive could be a reason for use while anything negative
could be a reason to discontinue use.
For each coding category, I separated the relevant statements into positive
statements and negative statements; if something good is said about a static
analysis tool it is considered a \emph{positive} comment and vice versa for a
\emph{negative} comment. In Figure~\ref{fig:results}, we can see that the
majority of participants have had problems with tool output, customizability
and workflow integration, and all but one of participant has had problems
with understanding the results of tool analyses. Tool design is not included because this category
was defined to capture the developers' ideas for improving static analysis
tools. Their reasons for wanting the features are captured in the other
categories.

\begin{figure*}
\centering
\includegraphics[scale=0.65,clip]{Chapter-3/figs/results.eps}
\caption{The number of participants in each category expressing the good and the bad about static analysis tools they have used.}
\label{fig:results}
\end{figure*}

\subsection{RQ1: Reasons for Use and Underuse}
\label{subsec:rq1}

The interviews revealed that there are a variety of reasons developers may have
for choosing to use a static analysis tool to find bugs in their code. One of
the obvious reasons is because too much time and effort is involved in manually
searching for bugs. Five out of 20 participants felt that because static
analysis tools can automatically find bugs, they are worth using. During his
interactive interview, Jason told us ``\emph{anything that will automate a
mundane task is great.}'' In other words, one reason for using static analysis
tools is that they automate the process of finding bugs.

Another reason developers might use a static analysis tool is if it is already
available in the development environment and ready to be used. For 3 participants, this was the case. 
Development environments such as IntelliJ and
PyCharm come with built-in static analyzers, which requires little extra effort
on the developer's part. Two participants, Matt and Adam, used PyCharm and
IntelliJ regularly and liked the fact that static analysis was already integrated.
For 7 participants, a good reason to use static analysis tools is to
support team development efforts. According to Josh and Andy, static analysis
tools do this by raising awareness of the potential problems, or ``dumb
mistakes,'' in the code earlier in the development process. For Cody and Ray,
static analysis tools are useful for communicating and enforcing coding
standards and styles on development teams. Some developers enjoy using the
static analysis tools they use to find bugs because of the level of
customizability. Three participants fit into this category. According to
James, the customizability of a tool can play a large part in the volume and
quality of output developers get.
% Cody likes using Dehydra~\cite{Dehydra} because it is easy to write new rules
% as well as modify the existing ones. <-- removed because not completely
% necessary, doesn't hurt anything when removed and for the sake of space

% ~\BIJ{I can put more here; I was trying to group them to show that there were
% things people agreed on but there were a few other things I could add}
Although some participants could find reasons to use static analysis
tools to find bugs, most of our participants brought up conflicting concerns
that could make the decision to adopt and regularly use static analysis tools less
obvious.

\textbf{Tool Ouput.} 
Tool output was a popular discussion topic. Out of the 20 developers I
interviewed, 14 expressed the negative impacts of poorly presented
output. Static analysis tools are known to produce false positives and these
false positives can ``outweigh'' the true positives in
volume~\cite{Shen:2011:EFindBugs}. Another known fact is that, especially with
larger projects, the number of warnings produced by a tool can be high,
sometimes in the thousands~\cite{Ayewah:2010:GFF}. Some participants
felt, however, that false positives and large volumes of warnings would be less
burdensome if the way the output is presented was more user-friendly and
intuitive. Cody, who likes using Dehydra, finds himself frustrated at times
because the results are dumped onto his screen with no distinct structure
causing him to spend a lot of time trying to figure out what needs to be done.
Jason wishes that his tool's output would be a ``slice'' that shows what the
problem is and what else could be affected in order to more quickly assess what
is or is not important. This ``slice'' should be taken from the entire project,
using call hierarchies, to show which parts are affected by each defect. During
his interactive interview he commented on a previous experience with FindBugs.
He had a large list of warnings to scroll through but without there being any
context to the problems it just seemed like ``a bunch of junk to sift through,''
which made him not want to bother using it. It may be worth investigating
how valuable an output like this would be.

\textbf{Collaboration.}
In industry, software development is often a team effort. For 9 participants, lack of or weak support for teamwork or collaboration was one
reason that teams, as well as individual developers, may not adopt or regularly
use static analysis tools.
According to John, although static analysis tools are useful for trying to
enforce coding standards, there is no easy way to share the settings with other
people on the team so it ends up being a cumbersome manual process and causing
confusion when the standards need to be changed. Many participants
mentioned the desire for a way to easily communicate and collaborate when using
their static analysis tool, especially in a team setting.
% For Matt, version control is more helpful when it comes to working and
% communicating on teams.
Although static analysis tools can be beneficial in team settings, current tools
are not collaborative enough for some developers. Newer versions of FindBugs
offer a cloud storage feature that can be used store, share and discuss warning
evaluations~\cite{FindBugsCloud}. Although a feature like this does make it
easier to communicate and share warning evaluations between developers, to add a
comment to a bug or current evaluation a web browser is needed. This takes the developer
out of context and out of the development environment which could demotivate
some individuals from checking them when they should.
% It may be worth investigating how to make static analysis tools more
% collaborative to support the sharing of information related to code analysis,
% such as coding standards or rule sets.

\textbf{Customizability.}
For 17 participants, customizability was important however many tools are
not trivial to configure and do not accommodate the customizations that
developers want. False positives and large volumes of warnings are well-known
downsides to using static analysis tool to find bugs. However, Frank told us he
believes that the way a tool is configured plays a large part in the output
a developer gets. John stated during his interview that ``\emph{many tools are so hard to
configure, they prevent you from doing anything.}'' Sometimes it is difficult
just to get to the menu where the options for configuring a particular feature
are, which participants Matt and Josh agree with. One participant, Jake,
found himself in an interesting situation during his interactive interview where
he could not figure out how to customize his tool and wound up having to search
the web to locate the tool's preferences. A common problem
expressed by most of the participants is the inability to temporarily ignore or
suppress certain warnings. Although some static analysis tools allow developers
to turn off certain filters, not all developers are comfortable with turning
warnings completely off. Matt, for example, is afraid that he may not remember
to turn it back on. The notion of dismissing or ignoring static analysis
warnings may be too coarse; as Jordan noted, he would prefer if static
analysis tools offered a way of recording his judgement about a given warning. More
sophisticated judgements may include things like ``this warning isn't a problem
now, but may be in the future if the following conditions are met\ldots''.

\textbf{Result Understandability.} 
The main objective when using a tool like FindBugs is to learn what defects are
in the code so that problems can be removed. A developer not being able to
understand what the tool is telling her, according to participants, is a
definite barrier to use. Nineteen out of 20 participants felt that many static
analysis tools do not present their results in a way that gives enough
information for them to assess what the problem is, why it is a problem and what
they should be doing differently. James told us during his interview that
``\emph{it's one thing to give an error message, it's another thing to give a
useful error message.}'' When talking about the Eclipse Python plug-ins, he also
stated, ``\emph{I find that the information they provide is not very useful, so
I tend to ignore them.}'' A few participants felt that it would be helpful to
have links to more details or examples in the error reports.
In some situations more information is needed to understand exactly what the
problem is and why it is a problem; understanding why a defect is a problem can
help the developer better assess whether the error is a false positive and try
to avoid repeating the same problem. Ryan told us during
his interactive interview that a start would be using ``real words,'' or a more
natural language, to explain the problem.

The most frequently mentioned difficulty when using static analysis tools was
lack of or ineffectively implemented quick fixes.
Most participants expressed interest in having their tool provide code
suggestions or quick fixes that assist them when attempting to fix a bug; Abby
proclaimed ``\emph{if you can tell me it's an error, you should be able to tell
me how to fix it.}'' Jordan strongly agrees; he loves tools that have quick
fixes and hates tools that do not. According to the interviews, these fixes do
not have to be automatic; some prefer that code suggestion previews be used or
possibly using examples to get a better understanding of how to fix the problem.
Some participants expressed interest in but skepticism toward integrating quick
fixes into static analysis tools. For example, during Jordan's interactive
interview, he noted that sometimes when using multiple tools, they may
have conflicting quick fixes or solutions. In Frank's past experiences with
automated code changes, he has had to do manual refactorings because something
was done wrong; because of this, he prefers to use find and replace to make
his own changes. Another participant, Adam, was concerned with knowing whether
the semantics of his code would be preserved after applying a quick fix. Most
static analysis tools, if they offer quick fixes, leave it to the developer to
figure out exactly what has been done after it has been done. Almost all of the participants agreed 
that effectively designed quick fixes can help them to better
understand the problems tools tell them about, leading to a better
sense of productivity for the developer.


\subsection{RQ2: Workflow Integration}
\label{subsec:rq2}

The most common topic during the interviews was
tool and environment integration. Sometimes a developer's process includes
running a static analysis tool, but more often it is not part of a
developer's workflow to stop and run a tool in the middle of working on some
code or a specific task; she usually prefers finding a ``stopping
point'' in her code to run the tool~\cite{Layman:2007:FaultFix}. Analysis of
these interviews revealed that while this is true, there are many different ways
that developers may want their tool to fit into their development workflow. For
example, some developers prefer that the tool run in the background; it
is easier for them to figure out what is wrong if they are in the process of
doing it and do not have to think about invoking the tool.
On the other hand, some developers do not
use IDEs, so if they are to use a static analysis tool, compiler
integration is very important. Nineteen of the 20 developers I interviewed
expressed the importance of workflow integration to them and how these needs have
been or should be met.

For some participants, there are features of static analysis tools they
have used that helped the tool better integrate into their workflow leading to
increased usage of the tool. In fact, John felt that
static analysis tools can be used to help organize a workflow, based on the
results it produces. For example, running a static analysis tool on
some code for the first time can be a good indicator of the kinds of bugs
the tool finds and that may be present; this can give an idea as to how detailed
of an analysis the tool provides, possibly giving a better idea of when it
would be best to run it. Of all the tools Adam has used in the past, he
much preferred to use IntelliJ and its built-in static analysis to find bugs; they
are tightly integrated making it seem more ``real time''. For these
participants, as well as a few others, integration with the development
environment plays a major role in their decision to use or continue using a
static analysis tool. Common standalone static analysis tools like FindBugs and
PMD have the ability to integrate with IDEs like Eclipse and NetBeans which
becomes especially important when using more than one static analysis
tool at a time, as we learned from discussing a past experience of Steve's where
he used 3 different static analysis tools at once. Jordan and
Chris like how FindBugs, PMD and CheckStyle fit into their
development processes; for Jordan, it is an integral part of his workflow. For the
majority of participants, however, current static analysis tools are not
doing enough to effectively integrate into their development process.

One of the biggest demotivational forces on a developer when it comes to using a
static analysis tool to find bugs is when it is what Tony calls a ``disjoint
process.'' Many participants, especially those who do not use IDEs, do
not like when they have to go out of their coding environment to use a tool or
view the results produced by the tool. For example, Frank, Lee, James and Andy
commented on how ``painful'' it was during their interactive interview to have
to switch perspectives in FindBugs to explore the complete
listing of bugs. According to Lee, having to open another perspective to know
what is going on is a guarantee that unmotivated people will not do it. For
Frank, although it is nice that the results are hidden so that you are not
overwhelmed, having to go back and forth and drill down to see the bugs requires
extra effort and is disruptive to his workflow. Other tools participants had
similar complaints about was Coverity and Lint for C/C++ projects. For Ryan and
Tony, the biggest downside to using Coverity is that it is not capable of being
integrated into their coding environment, leading to a lot of clicking back and
forth between their editor and the static analysis tool. Phil does not like
using Lint because of the fact that he has to ``go out of his way'' to do so.

Some participants made it clear, however, that even if the tool is
integrated with their development~\emph{environment}, it is still possible that
the tool does not integrate well into their development~\emph{process}. For
example, Mike does not use IDEs so using a tool that
integrates well with an IDE does not fit well into his development process; he
likes using Clang because it can be tied into his compiler which does not
require a ``development environment''. According to Gordon, one of the key
problems with static analysis tools is that at times they can prevent him from
being productive. One way this can happen is when the tool slows the developer
down by taking a long time to run, which was a common complaint amongst participants in this study. 
From Jason's experience, he believes that ``\emph{if it disrupts
your flow, you're not gonna use it.}'' Jason's statement rang true among other
participants as well, like Steve who had used various tools in his past but did 
not like to use FindBugs because, even though it is IDE integrable, it runs
slow. IntelliJ, which contains built in static analyzers, utilizes~\emph{idle
time} when reporting bugs in an attempt to prevent the problem of interrupting
the developer's workflow but for Matt, it can still be bothersome.
Jason believes that the problem with current static analysis tools is that they
are not capable of running well on larger code bases, leading to a break in his
``development flow'' as he waits for the tool to catch up.

In terms of workflow, participants valued using static analysis both to fix bugs
once they are introduced into the program, but also later in the development
process. From a workflow standpoint, it is valuable to fix potential bugs when
they are entered into a program because the necessary context to understand the
bug is already in the developers' working memory. In contrast, fixing bugs later
is difficult because a developer must recall the context to analyze the
corresponding static analysis warning. This contrast is similar to the
difference between ``floss refactoring'' and ``root canal refactoring,'' where
the former involves restructuring code as it is being worked with and the latter
involves refactoring by finding the ``worst code'' and dealing with that
first~\cite{Murphy-Hill:2008:RefactoringTools}.
Root canal refactoring is a discouraged practice and its analog in static
analysis -- finding the most severe static analysis warnings in a whole codebase
and dealing with those first -- may also be a wasteful practice. Research has
shown that many static analysis warnings in working systems do not actually
manifest as program failures~\cite{Ayewah:2010:GFF}.


\subsection{RQ3: Tool Design}
\label{subsec:rq3}

My main research goal to determine how we can improve static analysis tools for
developers. The best way to do this is to find out how developers want their
tool to be designed. Most of the proposed designs are for warning notification
and manipulation or quick fix display. Participants made some other interesting
proposals which will also be presented.

\textbf{Quick Fix Design.}
Ten participants made a suggestion related to the way in which a quick
fix should be displayed. Most participants wanted to be able to preview
the fix and how it is going to change their code before they apply it. Abby and
Tony recommended splitting the code editor to show a diff of the code, using
highlighting to show what code has changed or been added to their code. On one
side there would be the code now and on the other the code once the fix is
applied. Some felt that you should be able to see the fix before applying it,
but then also manually apply it so that you know the fix is being
applied without introducing any new problems.
One participant, Mike, prefers not to have quick fixes at all because he feels the error messages are enough to
assess what to do about an error.

% TODO talk about/cite FixBugs somewhere??
One interesting quick fix design idea, which came from Ryan during his
interactive interview, was to have what he called a ``\emph{three option dialog
box}'' available when applying a quick fix. This dialog box would pop up upon a
click to fix the bug and there would be three choices: apply the entire fix
(default option), do not apply the fix or step by step apply the solution
allowing the developer to decide which parts of the solution they would like to
keep. Static analysis tools like FindBugs and IntelliJ offer some quick fixes.
However, they do not give a full context preview of the changes that will be
made, leaving it to the developer to manually ensure that the fix was applied
correctly and to their liking.


\textbf{Warning Notification and Manipulation Design.} All 20 participants told us when and how they wanted to be notified of errors in their
code. The theme in this category is ``fast.'' Developers want tools that provide
faster feedback in an efficient way that does not disrupt their workflows. For
some participants, this meant running the tool in the background of the
IDE so that feedback occurs as soon as a problem is detected.
For other participants, this meant running the tool at build time or compile
time.  In this way, the results are presented when the developer is at a predefined ``stopping point."~\cite{Layman:2007:FaultFix}.

Overall, participants found that current static analysis tools are not fast
enough when providing them with feedback; this quickness should be accompanied
with discretion as the developer does not want the tool to break their thought
process.

Participants also thought it would be beneficial to have the ability to
easily make ``judgements'' about defects, such as setting it aside to view
later, save these judgements and share them with other developers. Many participants suggested that static analysis tools should allow developers to
ignore specific defects and move them to their own list for later viewing, a
form of \emph{temporary suppression}. Most tools, if they allow the developer to
ignore specific warnings, only allow the developer to turn off or suppress a bug
category for particular line of code using a comment-like annotation, which
Gordon told us makes the code ``smell''. Developers would like to have the
option to ignore each individual defect in case they either do not want to fix
it and do not want to be bothered by it again or do not want to be bothered with
it at that particular time but would like to come back to it later.

\textbf{Other Design Ideas.} Participants also came up with creative design
ideas. One participant, Chris, suggested giving the editor ``plasticity''. When
he is given a warning and would like to get more information, the tool should
move the code surrounding the warning to embed this information into the editor.
A couple of participants thought it would be useful to have visual output,
possibly a pie-style diagram of the project and the bugs in it, instead of
standard list and tree outputs to make it easier to go back and forth between
warnings and code. During Frank's participatory design session, he suggested a
potential solution; a parts-to-a-whole corpus view of the project as a
~\emph{``heat map''}. The heat map would use colors to show where the errors are
and how severe the problems are. It would start with an overall ``view'' of the
project and as you drill down you can see the condition at each level to see
where the most attention is needed. This is similar to the concept behind Khoo's
toolkit ~\emph{Path Projection} in that the toolkit is meant to visualize output
that is usually, if not always, textual and difficult to
understand~\cite{Khoo:2008:PathProjection}.


An interesting suggestion made by a couple of participants was to represent
the severity of the defects using gradients of one color instead of multiple
different colors; the darker the color the more important or urgent the bug is.
Figure~\ref{fig:participatory} depicts a drawing one participant, Matt,
drew during his participatory design; he labeled the side of the editor
``gradient'' (A) where he would like to see his severity representation. In the
top right corner, Matt also lists the colors that his current tool uses (B); for
example, ``R'' means red. The idea behind this is not new; other studies have
focused their attention on using colors for error
representation~\cite{Oberg:1992:Gradients, Murphy-Hill:2010:StenchBlossom}.
% Oberg and Notkin experimented with using graduated colors to notify the user
% of the \emph{age} of a defect; the darker the color the older the bug. This is
% similar to what our participants suggested except they want the color to
% represent how \emph{severe} the defect is. Murphy-Hill and Black propose a
% ``code smell'' detector that uses colors to indicate how obvious the code
% smells are in the code to the developer. If the code smell is one that the
% developer is likely to notice, the indicator is blue; if the code smell is one
% the developer is not likely to notice, the indicator is orange. Gradients
% between blue and orange are used for code smells that may be in between. This
% is also similar to what our participants suggested, however this approach uses
% gradients in between two different colors and the colors are used to represent
% the \emph{obviousness} of code smells.

\begin{figure}
\centering
\includegraphics[scale=0.5,clip]{Chapter-3/figs/participatory.eps}
\caption{One of our participant, Matt's, Participatory Design drawing; (A) shows where Matt wants the gradient colors and (B) shows the way his current tool represents severity.}
\label{fig:participatory}
\end{figure}

\subsection{Threats to Validity}

There exists threats to the validity of this study;
here I categorize each threat as a threat to external,
internal, or construct validity.

\textbf{External.} One limitation to the generalizability of this study is the
sample size. Although I obtained valuable information from the 20 interviews,
due to time constraints (and busy developers) they may not be representative of
the larger population that use static analysis tools. Although we would have
liked more participants, having a large number of interviews to transcribe and
code could lead to less accurate analysis. The study conducted by Layman et
al.~\cite{Layman:2007:FaultFix}, which we discussed earlier as utilizing a
similar methodology, had a participant pool of similar size (18).
Another possible threat is that we only interviewed developers who have used
static analysis tools. In some cases it may be that static analysis tools are
not being used for other reasons, such as lack of awareness. It should also be
noted that some participants had experience building static analysis
tools, giving them somewhat of a biased opinion of the usage of these tools.


\textbf{Internal.} Another threat to the validity of this study is the way in
which I conducted the remote interviews.
I did not thoroughly prepare for what I would do if the technology I wanted
to use did not work or was not available. Therefore, the interactive interview
and participatory design in remote interviews had to be conducted differently
than local interviews. Despite this, there was still value in the results
obtained from the remote participants; they could still give useful insights
from their previous experiences. Only 2 of the interviews fell into this
category, so this helps limit the impact of this threat.

\textbf{Construct.} The objective for using the interactive interview was to get
more accurate information on how developers use their tools. One limitation here
is that some developers were not as familiar with the code or environment they
had to use in the interviews as they would be with their own code in their own
development environment. This could have caused some developers to take
different actions than they would have in their own environment. Ideally
it would have been better to have been able to observe participants working
in their own environment; however, for confidentiality reasons, we could not view participants' proprietary code. 
In an effort to compensate for this threat, the open source projects and tool we chose are well-known, frequently used open
source projects. Another threat to the validity of this work is that I did not
originally consider that I may have said things in the consent form or
session script that would have given unintended ``hints'' to participants
concerning my research expectations. One example of this is my outlining the research goals in the introductions I gave prior to beginning each session.
This could have led to what is called ``hypothesis guessing'' where participants
respond to questions based on what they think the researcher wants to
hear~\cite{Threats}. In retrospect, I helped alleviate this threat in the interviews
by asking participants experience questions.

\section{Next Steps to A Solution}
% this study explored the what (what's causing developers to not frequently use tools). Most common findings = notification understandability. Next study/question that needs to be answered is why developers have difficulty with notifications -- only then can we start to explore ways to mitigate barriers and potentially increase usage/improve opinion of tools.
This study discovered reasons developers have for not using static analysis tools on their code.
Majority of participants noted having difficulty understanding and then coming up with a resolution for the notifications they encounter in their tools.
Based on the findings from this study, I discuss implications for improving developer perception of their tools.

% interactive quick fixes (help with resolution)
\subsection{Notification Resolution Solutions}

For many developers in the study above, it was important that their tools help them with resolving the defects found in their code.
Current static analysis tools may not give enough information for
developers to assess what to do about the warnings produced and very seldom
offer a fix to what it claims is an issue. If static analysis tools offered
quick fixes, giving a potential solution and applying it to the problem may help
developers assess warnings more quickly and ultimately save time and effort. My
results indicate that FindBugs, for example, would be more useful if it had more
informative messages and offered quick fixes. 

At the same time, quick fixes do not appear to be a universally applicable mechanism to help
developers resolve static analysis warnings because many static analysis
warnings do not have a small set of solutions. For example, FindBugs warns
developers when two method names in the same class differ only by
capitalization; no quick fix for this problem is likely to satisfy a developer.

Also, on the negative side, quick fixes could also cause
developers to be hasty in fixing their code, which could potentially lead to
more problems, such as the introduction of new defects~\cite{Mucslu:2012:Speculative}. 
There are also challenges related to implementing usable quick fixes. I
have not yet investigated what theses challenges are or how to address them as
they are out of scope for this dissertation. 
I instead, focus my attention on why developers have difficulty with interpreting tool notifications and how we can mitigate those challenges, thereby increasing developer ability to resolve notifications.

%Instead, interactive quick fixes that enable easy access to refactoring and code
%modification tools may be able to semi-automatically help developers resolve
%static analysis warnings. Barik and colleagues developed and evaluated an approach for interactively resolving defects based on these findings~\cite{barik2016quick}.

\subsection{Notification Understandability Solutions}
% notification understandability (help with understanding)
According to the findings above, although the point of using tools is to help identify and resolve defects, developers do not find the lack of quick fixes to be as much of a barrier to use as inability to interpret the notifications provided.
The tool, and any available quick fixes, become less useful if the developer does not understand the problem as it is being communicated.
% Based on the data from these interviews, some of the reasons developers do not frequently use static analysis tools are \textbf{Lack of collaborative support}, \textbf{Seemingly unorganized tool output}, \textbf{Poor support for customizations}, and \textbf{Difficult to interpret notifications}. 
Three out of four findings regarding tool use pertain to the notifications tools use and how tools present information to the developer.
Discovering that tool notifications are one of the reasons developers do not use program analysis tools is useful, however, not actionable. The next piece of information needed to make these findings actionable is to discover why developers have difficulty interpreting tool notifications. Only then can we start to explore ways to mitigate these barriers and potentially increase usage of these tools.
By focusing on improving developer ability to interpret tool notifications, I propose we can also improve developer ability to resolve tool notifications. In the next chapter, I discuss a follow-up study that explores why developer have difficulty with tool notifications.
